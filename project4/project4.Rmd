---
title: "CS598 - Project 3"
author: "Xiaoming Ji"
output: pdf_document
---
# Computer System

## Hardware
- Dell Precision Tower 5810
- CPU: Intel Xeon E5-1607 @ 3.10GHz
- Memory: 32GB
- GPU: Nvidia GeForce GTX 1080 (2 cards)

## Software
- OS: Windows 10 Professional 64bit
- R: 3.5.1
- R Packages:
    - text2vec
    - tokenizers
    - xgboost_0.71.2
    - glmnet_2.0-16

```{r, message=FALSE, warning=FALSE, include=FALSE}
mypackages = c("knitr", "kableExtra", "text2vec", "tokenizers",
               "glmnet", "xgboost")   # required packages
tmp = setdiff(mypackages, rownames(installed.packages()))  # packages need to be installed
if (length(tmp) > 0) install.packages(tmp)
lapply(mypackages, require, character.only = TRUE)

SKIPMAIN = TRUE
source("mymain.R")
```

# Overview

The goal of this project is to build a binary classification model to predict the sentiment of IMDB movie review.

The given movie review dataset file `Project4_data.tsv` has 50,000 rows (i.e., reviews) and 3 columns. Column 1 "new_id" is the ID for each review (same as the row number), Column 2 "sentiment" is the binary response, and Column 3 is the review. 

The given dataset file `Project4_splits.csv` is used to split the 50,000 reviews into Three sets of training and test data.  The dataset has 3 columns, and each column consists of the "new_id" (or equivalently the row ID) of 25,000 test samples. 

The required performance goal is is to achieve minimal value of AUC over the three test datasets. For model interpretability, the vocabulary size should less than 3000.

For this project, I used logistic linear/lasso regression model to select vocabulary and logistic linear/ridge regression model for training/prediction. Such approach achieved `0.965` AUC with only `1000` vocabulary size.

```{r, eval=FALSE, include=FALSE}
all = read.table("data.tsv",stringsAsFactors = F,header = T)
all$new_id = as.integer(all$new_id)
all$sentiment = as.integer(all$sentiment)
all$review = gsub('<.*?>', ' ', all$review)  #Remove HTML tag
all$review = gsub('[^[:alnum:]]', ' ', all$review) #Remove punctuation
splits = read.table("splits.csv", header = T)

stop_words = c("i", "me", "my", "myself", 
               "we", "our", "ours", "ourselves", 
               "you", "your", "yours", 
               "their", "they", "his", "her", 
               "she", "he", "a", "an", "and",
               "is", "was", "are", "were", 
               "him", "himself", "has", "have", 
               "it", "its", "of", "one", "for", 
               "the", "us", "this")

it.all = itoken(all$review, 
                preprocessor = tolower, 
                tokenizer = word_tokenizer, #tokenize_word_stems, 
                ids = all$id, 
                progressbar = FALSE)
  
all.vocab = create_vocabulary(it.all, stopwords = stop_words, ngram = c(1L, 4L))
all.vocab = prune_vocabulary(all.vocab, term_count_min = 5,
                               doc_proportion_max = 0.5,
                               doc_proportion_min = 0.001)
```

# Build Vocabulary

Building vocabulary is basically a feature selection process. I first build the vocabulary using all 50,000 reviews with tokenization process as: 

- Filter out HTML tag and non-alphanumeric characters.
- Remove stop words:

> `i`, `me`, `my`, `myself`, `we`, `our`, `ours`, `ourselves`, `you`, `your`, 
> `yours`, `their`, `they`, `his`, `her`, `she`, `he`, `a`, `an`, `and`, `is`, 
> `was`, `are`, `were`, `him`, `himself`, `has`, `have`, `it`, `its`, `of`, 
> `one`, `for`, `the`, `us`, `this`

-  Extract 1 to 4 ngram terms

Then I prune this vocabulary with conditions: 

- minimum 5 occurences over all documents.
- minimum 0.1% of documents which should contain this term.
- maximum 50% of documents which should contain this term.

This gives me a vocabulary size of `29035`.

I have tried several approaches to reduce the size of the vocabulary:

1. Normalized Difference Sentiment Index as described in [A Quick R Demonstration on Sentiment Analysis and Textmining](https://drive.google.com/file/d/0B_sqyEYBKc1wVm4xN0NvQlJlNWc/view)
2. [Discriminant Analysis for Two-sample T-test ](https://piazza.com/class/jky28ddlhmu2r8?cid=663) as illustrated by Professor. Feng Liang.
3. Use (XG)Boosting to build a model on all review data and select features/terms using `xgb.importance()` ordered by Gain.
4. Use logistic linear regression model with Lasso regularization to fit the review data, then select the none-zero cofficient term (3200+). For these terms, sort by the absolute value of coefficient, then select the 1000, 2000, 3000 terms as vocabulary candidates.

My testing shows approach #3 and #4 work better than #1 and #2. #4 works extremely well and I can achieve above `0.965` AUC using `1000` terms. Adding more terms results in bigger AUC (see the performance number in next section). 

*Note:* I also tried tokenization with word stemming but got lower AUC with same vocabulary size. Thus, no word stemming is used.

# Classification Model: type of algorithms/models used/explored, pre-processing, training process, tuning parameters, etc.

For classification model, I used logistic linear regression with Ridge regularization. `cv.glmnet` is used to select the minminal $\lambda$ to build the model.

Three datasets are tested on vocabulary of size 1000, 2000 and 3000 respetively. And I get the performance matrix as:

```{r, eval=FALSE, include=FALSE}
start.time = Sys.time()

VOCAL_SIZE = c(1000, 2000, 3000)
auc.matrix = matrix(0, 4, length(VOCAL_SIZE))
rownames(perf.matrix) = c("Performance for Split 1", "Performance for Split 2", 
                          "Performance for Split 3", "Vocab Size")
auc.matrix[4,] = VOCAL_SIZE

for (v in 1:length(VOCAL_SIZE)){
  vocab.size = VOCAL_SIZE[v]
  my.vocab = trim_vocab_by_lasso(all, all.vocab, vocab.size)
  for (s in 1:3){
    train = all[-which(all$new_id%in%splits[,s]),]
    test = all[which(all$new_id%in%splits[,s]),]
    auc.matrix[s, v] = make_prediction(my.vocab, train, test)[[1]]$auc
  }
  
  f = file(paste("myVocab_", vocab.size, ".txt", sep=""), open = "w")
  writeLines(sort(my.vocab), f)
  close(f)
}

print(auc.matrix)

end.time = Sys.time()
run.time = as.numeric(difftime(end.time, start.time, units = 'secs'))

eval.obj = list(perf=auc.matrix, compute.time=run.time)

save(eval.obj, file="EVAL.OBJ")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
load("EVAL.OBJ")

kable(eval.obj$perf, booktabs = T) %>%
  row_spec(4, bold = T, color = "white", background = "black")
```

Computation time: `r ceiling(eval.obj$compute.time)` seconds

# Discussion on model limitations, explanation on errors, and future steps you could take to improve your model.
