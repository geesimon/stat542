---
title: "CS598 - Project 3"
author: "Xiaoming Ji"
output: pdf_document
---
# Computer System

## Hardware
- Dell Precision Tower 5810
- CPU: Intel Xeon E5-1607 @ 3.10GHz
- Memory: 32GB
- GPU: Nvidia GeForce GTX 1080 (2 cards)

## Software
- OS: Windows 10 Professional 64bit
- R: 3.5.1
- R Packages:
    - text2vec
    - e1071/naiveBayes
    - tokenizers
    - catboost_0.11.1
    - xgboost_0.71.2
    - randomForest_4.6-14
    - glmnet_2.0-16
    - kernlab_0.9-26

```{r, message=FALSE, warning=FALSE, include=FALSE}
mypackages = c("knitr", "kableExtra", "text2vec", "tokenizers","glmnet")   # required packages
tmp = setdiff(mypackages, rownames(installed.packages()))  # packages need to be installed
if (length(tmp) > 0) install.packages(tmp)
lapply(mypackages, require, character.only = TRUE)

source("mymain.R")
```


```{r, eval=FALSE, include=FALSE}
  all = read.table("data.tsv",stringsAsFactors = F,header = T)
  all$new_id = as.integer(all$new_id)
  all$sentiment = as.integer(all$sentiment)
  all$review = gsub('[^[:alnum:]]', ' ', all$review)
  splits = read.table("splits.csv", header = T)
```
```{r}
  s = 2
  
  train = all[-which(all$new_id%in%splits[,s]),]
  test = all[which(all$new_id%in%splits[,s]),]
  
  # f= file("stopwords.txt")
  # stop_words = readLines(f)
  # close(f)
  stop_words = c("i", "me", "my", "myself", 
               "we", "our", "ours", "ourselves", 
               "you", "your", "yours", 
               "their", "they", "his", "her", 
               "she", "he", "a", "an", "and",
               "is", "was", "are", "were", 
               "him", "himself", "has", "have", 
               "it", "its", "of", "one", "for", 
               "the", "us", "this")
```


```{r}
all.vocab = build_vocab(all)

all.vocab.w = all.vocab

for (i in 0:10){
  max.word.count = 1000 + i * 100
  for(s in 1:3){
    train = all[-which(all$new_id%in%splits[,s]),]
    test = all[which(all$new_id%in%splits[,s]),]
    
    w = make_prediction(all.vocab.w, train, test)
    w.data = all[which(all$new_id%in%w$WrongIDs),]
    w.vocab = build_vocab(w.data, word.count = max.word.count)
    all.vocab.w = c(all.vocab.w, w.vocab[which(!(w.vocab %in% all.vocab.w))])

    cat("S=", s, "AUC=", w$AUC, "Wrong=", length(w$WrongIDs),"VocabSize=", 
        length(all.vocab.w), "\n")
  }
}

f= file("myVocab.txt", open = "w")
writeLines(all.vocab.w, f)
close(f)

# w1.data = all[which(all$new_id%in%w1$WrongIDs),]
# w1.vocab = build_vocab(w1.data)
# all.vocab.w1 = c(all.vocab, w1.vocab[which(!(w1.vocab %in% all.vocab))])
# w2 = make_prediction(all.vocab.w1, train, test)
# 
# 
# w2.data = all[which(all$new_id%in%w2$WrongIDs),]
# w2.vocab = build_vocab(w2.data)
# all.vocab.w2 = c(all.vocab.w1, w2.vocab[which(!(w2.vocab %in% all.vocab.w1))])
# w3 = make_prediction(all.vocab.w2, train, test)
```

```{r}
t1 = Sys.time()
it_train = itoken(train$review, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = train$id, 
             progressbar = FALSE)

# Note that most text2vec functions are pipe friendly!
it_test = itoken(test$review, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = train$id, 
             progressbar = FALSE)

vectorizer = vocab_vectorizer(vocab)
# create dtm_train with new pruned vocabulary vectorizer

dtm_train  = create_dtm(it_train, vectorizer)
#dtm_train_l1_norm = normalize(dtm_train, "l1")

# define tfidf model
tfidf = TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_train_tfidf = fit_transform(dtm_train, tfidf)
# tfidf modified by fit_transform() call!
# apply pre-trained tf-idf transformation to test data
dtm_test = create_dtm(it_test, vectorizer)
dtm_test_tfidf = transform(dtm_test, tfidf)

NFOLDS = 5
my.cv = cv.glmnet(x = dtm_train, y = train[['sentiment']], 
                              family = 'binomial', 
                              # L1 penalty
                              alpha = 0,
                              # interested in the area under ROC curve
                              type.measure = "auc",
                              # 5-fold cross-validation
                              nfolds = NFOLDS)
                              # # high value is less accurate, but has faster training
                              # thresh = 1e-3,
                              # # again lower number of iterations for faster training
                              # maxit = 1e3)
my.fit = glmnet(x = dtm_train, y = train$sentiment, 
               lambda = my.cv$lambda.min, family='binomial', alpha=0)

preds = predict(my.fit, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)


print(difftime(Sys.time(), t1, units = 'sec'))
```
```{r}
```

## Preprocessing and Feature Engineering

Several approaches are taken to pre-process the data.

- Response label: merge *Charged Off* to *Default* and convert the label value to 0 or 1.
- Build new predictors to help training/prediction: 
    - `earliest_cr_line_mon`: derived from *earliest_cr_line* that indicates how many months has elapsed till *2019-1-1* when the borrower's earliest reported credit line was opened.
    - `fico_score`: consolidate *fico_range_high* and *fico_range_low* using formula: *(fico_range_high + fico_range_low) / 2*.
- Level grouping: 
    - `zip_code`: it has more than 900 levels, I group these values to 10 new levels to reduce memory usage and improve performance.
- Remove predictors: remove less useful and redundant predictors.
    - `emp_title` (too many levels), `title` (redundant with *purpose*), `grade` (redundant with *sub_grade*) ,`earliest_cr_line`, `fico_range_high`, `fico_range_low`.

## Models

For testing purpose, I build 7 models,

- Dumb model: this is the simplest model that predict 0.2 for every sample.
- Logistic Regression
- Boosting (XGBoost, CatBoost)
- RandomForest
- Lasso
- liner SVM

Suprisingly, Dumb model can achieve `0.504` logloss score. kernlab *ksvm()* fails to build the model (hang forever).  lasso and random forest don't give me significant improvement than logistic regression and they take much longer time to build. Thus, I will pick `dumb`, `Logistic Regression` and `Boosting` as my final models. 

**Note**: My testing shows CatBoost performs at least 10x faster than XGBoost (with GPU, CatBoost can do even better). In case catboost library is not installed, xgboost will be used.

## Evaluation

I tested all 3 test datasets against these models with the parameters,

- Dumb: None.
- Logistic Regression: liner combination of all available predictors.
- Boosting: One-hot encoding on train/test data then train with the following parameters,
    - CatBoost: loss_function = "Logloss", learning_rate = 0.09, iterations = 1200
    - XGBoost: objective = "binary:logistic", eval_metric = "logloss", eta = 0.09, nrounds = 1200

The LogLoss scores are,

```{r, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
load("EVAL.OBJ")

test.results = rbind(eval.obj$loss, Average=colMeans(eval.obj$loss))
kable(test.results) %>%
  kable_styling(latex_options = "striped")
```

```{r eval=FALSE, eval=FALSE, warning=FALSE, include=FALSE}
#########################################################################
# Test code begins

start.time = Sys.time()

LOGLOSS = matrix(0, 3, 3)
rownames(LOGLOSS) = c("Test1", "Test2", "Test3")
TEST_NUM = 0

for (i in 1:3){
  TEST_NUM = i
  TRAIN_FILE_NAME = paste("train",i, ".csv", sep = "")
  TEST_FILE_NAME = paste("test",i, ".csv", sep = "")
  LABEL_FILE_NAME = paste("label",i, ".csv", sep = "")
  source('mymain.R')
}
end.time = Sys.time()
run.time = as.numeric(difftime(end.time, start.time, units = 'secs'))

print(LOGLOSS)
cat("\nComputation time:", ceiling(run.time), "Seconds")

eval.obj = list(loss=LOGLOSS, compute.time=run.time)
save(eval.obj, file="EVAL.OBJ")
```

Computation time: `r ceiling(eval.obj$compute.time)` seconds