---
title: "CS598 - Project 3"
author: "Xiaoming Ji"
output: pdf_document
---
# Computer System

## Hardware
- Dell Precision Tower 5810
- CPU: Intel Xeon E5-1607 @ 3.10GHz
- Memory: 32GB
- GPU: Nvidia GeForce GTX 1080 (2 cards)

## Software
- OS: Windows 10 Professional 64bit
- R: 3.5.1
- R Packages:
    - text2vec
    - tokenizers
    - xgboost_0.71.2
    - glmnet_2.0-16

```{r, message=FALSE, warning=FALSE, include=FALSE}
mypackages = c("knitr", "kableExtra", "text2vec", "tokenizers",
               "glmnet", "xgboost")   # required packages
tmp = setdiff(mypackages, rownames(installed.packages()))  # packages need to be installed
if (length(tmp) > 0) install.packages(tmp)
lapply(mypackages, require, character.only = TRUE)

source("mymain.R")
```

# Overview of your model: background, problem statement, objective, input and output of your model, ect.

The goal of this project is to build a binary classification model to predict the sentiment of a IMDB movie review.

The given movie review dataset file `Project4_data.tsv` has 50,000 rows (i.e., reviews) and 3 columns. Column 1 "new_id" is the ID for each review (same as the row number), Column 2 "sentiment" is the binary response, and Column 3 is the review. 

The given dataset file `Project4_splits.csv` is used to split the 50,000 reviews into Three sets of training and test data.  The dataset has 3 columns, and each column consists of the "new_id" (or equivalently the row ID) of 25,000 test samples. 

The required performance goal is is to achieve minimal value of AUC over the three test datasets and use the vocabulary of size less than 3000.

For this project, I used linear/lasso model to select the vocabulary at size of `1500` and logistic linear/ridge regression to achieved $>0.97$ value of AUC.

```{r, eval=FALSE, include=FALSE}
all = read.table("data.tsv",stringsAsFactors = F,header = T)
all$new_id = as.integer(all$new_id)
all$sentiment = as.integer(all$sentiment)
all$review = gsub('<.*?>', ' ', all$review)  #Remove HTML tag
all$review = gsub('[^[:alnum:]]', ' ', all$review) #Remove punctuation
splits = read.table("splits.csv", header = T)

stop_words = c("i", "me", "my", "myself", 
               "we", "our", "ours", "ourselves", 
               "you", "your", "yours", 
               "their", "they", "his", "her", 
               "she", "he", "a", "an", "and",
               "is", "was", "are", "were", 
               "him", "himself", "has", "have", 
               "it", "its", "of", "one", "for", 
               "the", "us", "this")

it.all = itoken(all$review, 
                preprocessor = tolower, 
                tokenizer = word_tokenizer, #tokenize_word_stems, 
                ids = all$id, 
                progressbar = FALSE)
  
all.vocab = create_vocabulary(it.all, stopwords = stop_words, ngram = c(1L, 4L))
all.vocab = prune_vocabulary(all.vocab, term_count_min = 5,
                               doc_proportion_max = 0.5,
                               doc_proportion_min = 0.001)
```

# Customized vocabulary: explain how you obtain your customized vocab if applicable.

Vocab by Lasso
```{r}
vectorizer = vocab_vectorizer(all.vocab)
dtm.all  = create_dtm(it.all, vectorizer)
  
NFOLDS = 5
my.cv = cv.glmnet(x = dtm.all, y = all$sentiment, 
                    family = 'binomial', 
                    alpha = 1,
                    type.measure = "auc",
                    nfolds = NFOLDS)

my.fit = glmnet(x = dtm.all, y = all$sentiment, 
                  lambda = my.cv$lambda.1se, family='binomial', alpha = 1)

betas = coef(my.fit)[-1, 1] #Remove inception
vocab.id = order(abs(betas), decreasing=TRUE)[1:500]
vocab = all.vocab$term[vocab.id]

f= file(VOCAB_FILE, open = "w")
writeLines(sort(vocab), f)
close(f)
```

Vocab by XGBoost
```{r}
vectorizer = vocab_vectorizer(all.vocab)

X_train = create_dtm(it.all, vectorizer)
Y_train = all$sentiment
  
xgb.model = xgboost(data = X_train, label=Y_train,
                      objective = "binary:logistic", eval_metric = "auc",
                      eta = 0.09,
                      nrounds = 1642,
                      # colsample_bytree = 0.6,
                      # subsample = 0.75,
                      verbose = TRUE)

# cv <- xgb.cv(data = X_train, label = Y_train,
#               objective = "binary:logistic", eval_metric = "auc",
#               early_stopping_rounds = 10,
#               # max_depth = 6,
#               nfold = 5, nrounds = 2000,
#               eta = 0.09,
#               verbose = TRUE)

feature.importance = xgb.importance(model= xgb.model)
vocab = feature.importance$Feature[1:2000]

f= file(VOCAB_FILE, open = "w")
writeLines(sort(vocab), f)
close(f)
```

# Technical details: type of algorithms/models used/explored, pre-processing, training process, tuning parameters, etc.

# Model validation: performance on the three test datasets, discussion on model limitations, explanation on errors, and future steps you could take to improve your model.

```{r}
t1 = Sys.time()

f= file(VOCAB_FILE)
my.vocab = readLines(f)
close(f)

result = list()
for (s in 1:3){
  train = all[-which(all$new_id%in%splits[,s]),]
  test = all[which(all$new_id%in%splits[,s]),]
  result[[s]] = make_prediction(my.vocab, train, test) #, tokenize_word_stems)
}

print(result)

print(difftime(Sys.time(), t1, units = 'sec'))
```


## Preprocessing and Feature Engineering

Several approaches are taken to pre-process the data.

- Response label: merge *Charged Off* to *Default* and convert the label value to 0 or 1.
- Build new predictors to help training/prediction: 
    - `earliest_cr_line_mon`: derived from *earliest_cr_line* that indicates how many months has elapsed till *2019-1-1* when the borrower's earliest reported credit line was opened.
    - `fico_score`: consolidate *fico_range_high* and *fico_range_low* using formula: *(fico_range_high + fico_range_low) / 2*.
- Level grouping: 
    - `zip_code`: it has more than 900 levels, I group these values to 10 new levels to reduce memory usage and improve performance.
- Remove predictors: remove less useful and redundant predictors.
    - `emp_title` (too many levels), `title` (redundant with *purpose*), `grade` (redundant with *sub_grade*) ,`earliest_cr_line`, `fico_range_high`, `fico_range_low`.

## Models

For testing purpose, I build 7 models,

- Dumb model: this is the simplest model that predict 0.2 for every sample.
- Logistic Regression
- Boosting (XGBoost, CatBoost)
- RandomForest
- Lasso
- liner SVM

Suprisingly, Dumb model can achieve `0.504` logloss score. kernlab *ksvm()* fails to build the model (hang forever).  lasso and random forest don't give me significant improvement than logistic regression and they take much longer time to build. Thus, I will pick `dumb`, `Logistic Regression` and `Boosting` as my final models. 

**Note**: My testing shows CatBoost performs at least 10x faster than XGBoost (with GPU, CatBoost can do even better). In case catboost library is not installed, xgboost will be used.

## Evaluation

I tested all 3 test datasets against these models with the parameters,

- Dumb: None.
- Logistic Regression: liner combination of all available predictors.
- Boosting: One-hot encoding on train/test data then train with the following parameters,
    - CatBoost: loss_function = "Logloss", learning_rate = 0.09, iterations = 1200
    - XGBoost: objective = "binary:logistic", eval_metric = "logloss", eta = 0.09, nrounds = 1200

The LogLoss scores are,

```{r, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
load("EVAL.OBJ")

test.results = rbind(eval.obj$loss, Average=colMeans(eval.obj$loss))
kable(test.results) %>%
  kable_styling(latex_options = "striped")
```

Computation time: `r ceiling(eval.obj$compute.time)` seconds