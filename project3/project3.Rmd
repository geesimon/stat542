---
title: "CS598 - Project 3"
author: "Xiaoming Ji"
output: pdf_document
---
# Computer System

## Hardware
- Dell Precision Tower 5810
- CPU: Intel Xeon E5-1607 @ 3.10GHz
- Memory: 32GB
- GPU: Nvidia GeForce GTX 1080 (2 cards)

## Software
- OS: Windows 10 Professional 64bit
- R: 3.5.1
- R Packages:
    - randomForest_4.6-14
    - glmnet_2.0-16
    - xgboost_0.71.2
    - kernlab_0.9-26
    - catboost_0.11.1

```{r, message=FALSE, warning=FALSE, include=FALSE}
mypackages = c("knitr", "kableExtra")   # required packages
tmp = setdiff(mypackages, rownames(installed.packages()))  # packages need to be installed
if (length(tmp) > 0) install.packages(tmp)
lapply(mypackages, require, character.only = TRUE)
```


```{r, eval=FALSE, include=FALSE}
data.all = read.csv("loan_stat542.csv")
test.id.all = read.csv("Project3_test_id.csv")

for (i in 1:(dim(test.id.all)[2])){
  
  train.data = data.all[!data.all$id %in% test.id.all[,i],]
  write.csv(train.data, file=paste("train", i, ".csv", sep=""), row.names = FALSE)
  test.data = data.all[data.all$id %in% test.id.all[,i],]
  
  label.data = test.data[,c("id", "loan_status")]
  colnames(label.data)[2] = 'y'
  label.data$y[label.data$y == "Charged Off"] = "Default"
  label.data$y = as.numeric(label.data$y)
  label.data$y[label.data$y == 3] = 0
  label.data$y[label.data$y == 2] = 1
  write.csv(label.data, file=paste("label", i, ".csv", sep=""), row.names = FALSE)
  
  test.data = test.data[, c("loan_status") != names(test.data)]
  write.csv(test.data, file=paste("test", i, ".csv", sep=""), row.names = FALSE)
}
```

## Preprocessing and Feature Engineering

Several approaches are taken to pre-process the data.

- Response label: merge *Charged Off* with *Default* and convert label values to 0 or 1.
- Build new predictors to help training/prediction: 
    - `earliest_cr_line_mon`: derived from *earliest_cr_line* that indicates how many months has elapsed till *2019-1-1* when the borrower's earliest reported credit line was opened.
    - `fico_score`: consolidate *fico_range_high* and *fico_range_low* using formula: *(fico_range_high + fico_range_low) / 2*.
- Level grouping: 
    - `zip_code`: it has more than 900 levels, I group these values to 10 new levels to reduce memory usage and improve performance.
- Remove predictors: remove less useful and redundant predictors.
    - `emp_title` (too many levels), `title` (redundant with *purpose*), `grade` (redundant with *sub_grade*) ,`earliest_cr_line`, `fico_range_high`, `fico_range_low`.

## Models

For testing purpose, I build 7 models,

- Dumb model: just predict 0.2 for every sample.
- Logistic Regression
- Boosting (Xgboost, CatBoost)
- RandomForest
- Lasso
- liner SVM

Suprisingly, Dumb model can achieve `0.504` logloss score. kernlab *ksvm()* fails to build the model (hang forever).  lasso and random forest don't give me significant improvement than logistic regression and they take much longer time to build. Thus, I will pick `dumb`, `Logistic Regression` and `Boosting` as my final models. 

**Note**: My testing shows catboost performs at least 10x faster than xgboost (even without GPU). However, given catboost is not in standard CRAN repository and I'm worried it may have difficulty to be installed in the grading environment, I will still use xgboost to build the boosting model.

## Evaluation

I tested all 3 test datasets against these models with the parameters,

- Dumb: None
- Logistic Regression: liner with all predictors.
- Xgboost: max_depth = 6, eta = 0.1, nrounds = 1200, colsample_bytree = 0.6, subsample = 0.75.

The LogLoss scores are,

```{r, echo=FALSE, message=FALSE, warning=FALSE}
load("EVAL.OBJ")

test.results = rbind(eval.obj$loss, Average=colMeans(eval.obj$loss))
kable(test.results) %>%
  kable_styling(latex_options = "striped")
```

```{r eval=FALSE, warning=FALSE, include=FALSE}
#########################################################################
# Test code begins

start.time = Sys.time()

LOGLOSS = matrix(0, 3, 3)
rownames(LOGLOSS) = c("Test1", "Test2", "Test3")
TEST_NUM = 0

for (i in 1:3){
  TEST_NUM = i
  TRAIN_FILE_NAME = paste("train",i, ".csv", sep = "")
  TEST_FILE_NAME = paste("test",i, ".csv", sep = "")
  LABEL_FILE_NAME = paste("label",i, ".csv", sep = "")
  source('mymain.R')
}
end.time = Sys.time()
run.time = as.numeric(difftime(end.time, start.time, units = 'secs'))

print(LOGLOSS)
cat("\nComputation time:", ceiling(run.time), "Seconds")

eval.obj = list(loss=LOGLOSS, compute.time=run.time)
save(eval.obj, file="EVAL.OBJ")
```

Computation time: `r ceiling(eval.obj$compute.time)` seconds

```{r eval=FALSE, include=FALSE}
# # submission files
# allFiles = list.files()
# subFiles = grep('mysubmission', allFiles, value = TRUE, 
#                 ignore.case = TRUE)
# 
# # calculate the test error on the test set
# test = read.csv('test.csv')
# 
# label = read.csv('label.csv', sep = ',')
# err = rep(NA, length(subFiles))
# for (met in 1:length(subFiles)){
# 
#     prediction = read.csv(subFiles[met], sep = ',')
#     yp = merge(prediction, label, by = 'id', all.y = TRUE)
#     err[met] = with(yp, logLoss(y, prob))
#     
# }
# 
# #########################################################################
# write.table(err, file = 'proj_3.csv', sep = ',', row.names = FALSE,
#             col.names = FALSE)
# write.table(run.time, file = 'proj_3.csv', sep = ',', 
#             row.names = FALSE, col.names = FALSE, append = TRUE)
```

