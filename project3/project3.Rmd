---
title: "CS598 - Project 3"
author: "Xiaoming Ji"
output: pdf_document
---
# Computer System

## Hardware
- Dell Precision Tower 5810
- CPU: Intel Xeon E5-1607 @ 3.10GHz
- Memory: 32GB
- GPU: Nvidia GeForce GTX 1080 (2 cards)

## Software
- OS: Windows 10 Professional 64bit
- R: 3.5.1
- R Packages:
    - randomForest_4.6-14
    - glmnet_2.0-16
    - xgboost_0.71.2
    - kernlab_0.9-26
    - catboost_0.11.1

```{r, eval=FALSE, include=FALSE}
data.all = read.csv("loan_stat542.csv")
test.id.all = read.csv("Project3_test_id.csv")

for (i in 1:(dim(test.id.all)[2])){
  
  train.data = data.all[!data.all$id %in% test.id.all[,i],]
  write.csv(train.data, file=paste("train", i, ".csv", sep=""), row.names = FALSE)
  test.data = data.all[data.all$id %in% test.id.all[,i],]
  
  label.data = test.data[,c("id", "loan_status")]
  colnames(label.data)[2] = 'y'
  label.data$y[label.data$y == "Charged Off"] = "Default"
  label.data$y = as.numeric(label.data$y)
  label.data$y[label.data$y == 3] = 0
  label.data$y[label.data$y == 2] = 1
  write.csv(label.data, file=paste("label", i, ".csv", sep=""), row.names = FALSE)
  
  test.data = test.data[, c("loan_status") != names(test.data)]
  write.csv(test.data, file=paste("test", i, ".csv", sep=""), row.names = FALSE)
}
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
mypackages = c("knitr")   # required packages
tmp = setdiff(mypackages, rownames(installed.packages()))  # packages need to be installed
if (length(tmp) > 0) install.packages(tmp)
lapply(mypackages, require, character.only = TRUE)
```

## Preprocessing and Feature Engineering

Several approaches are taken to pre-process the data.

- Response label: merge *Charged Off* with *Default* and convert it to 0/1.
- Build new predictors to help training/prediction: 
    - `earliest_cr_line_mon`: derived from *earliest_cr_line* that indicates how many months has elapsed till *2019-1-1* when the borrower's earliest reported credit line was opened. Then use this predictor instead of earliest_cr_line to build the model.
    - `fico_score`: consolidate *fico_range_high* and *fico_range_low* using formula: *(fico_range_high + fico_range_low) / 2*.
- Level grouping: 
    - `zip_code`: it has more than 900 values, I group these values to 10 new levels to reduce memory usage and improve performance.
- Remove predictors: remove less useful and redundant predictors.
    - `emp_title`, `title`, `grade` (redundant with *sub_grade*) ,`earliest_cr_line`, `fico_range_high`, `fico_range_low`.

## Models

For testing purpose, I build 7 models,

- Dumb model: just predict 0.2 for every sample.
- Logistic Regression
- Boosting (Xgboost, catboost)
- RandomForest
- Lasso
- liner SVM

Suprisingly, Dumb model can achive `0.504` logloss score. kernlab *ksvm()* fails to build the model (hang forever).  Lasso and RandomForest can't make significant better results than Logistic regression although they take much longer time to build. Thus, I will use `dumb`, `Logistic Regression` and `Boosting` models. 

**Note**: My testing shows catboost performs at least 10x faster than xgboost when building the model (even without GPU). However, given catboost is not in CRAN repository and I'm worried it may have difficulty to be installed in the grading env, I will still use xgboost to build the boosting model.

## Evaluation

I tested all 3 test datasets against these models with the parameters,

- Dumb: None
- Logistic Regression: liner with all predictors.
- Xgboost: max_depth = 6, eta = 0.1, nrounds = 1200, colsample_bytree = 0.6, subsample = 0.75.

The LogLoss scores are,

```{r, echo=FALSE, message=FALSE, warning=FALSE}
load("EVAL.OBJ")

test.results = rbind(eval.obj$loss, Average=colMeans(eval.obj$loss))
kable(test.results)
```

```{r eval=FALSE, warning=FALSE, include=FALSE}
#########################################################################
# Test code begins

start.time = Sys.time()

LOGLOSS = matrix(0, 3, 3)
rownames(LOGLOSS) = c("Test1", "Test2", "Test3")
TEST_NUM = 0

for (i in 1:3){
  TEST_NUM = i
  TRAIN_FILE_NAME = paste("train",i, ".csv", sep = "")
  TEST_FILE_NAME = paste("test",i, ".csv", sep = "")
  LABEL_FILE_NAME = paste("label",i, ".csv", sep = "")
  source('mymain.R')
}
end.time = Sys.time()
run.time = as.numeric(difftime(end.time, start.time, units = 'secs'))

print(LOGLOSS)
cat("\nComputation time:", ceiling(run.time), "Seconds")

eval.obj = list(loss=LOGLOSS, compute.time=run.time)
save(eval.obj, file="EVAL.OBJ")
```

Computation time: `r ceiling(eval.obj$compute.time)` seconds

```{r eval=FALSE, include=FALSE}
# # submission files
# allFiles = list.files()
# subFiles = grep('mysubmission', allFiles, value = TRUE, 
#                 ignore.case = TRUE)
# 
# # calculate the test error on the test set
# test = read.csv('test.csv')
# 
# label = read.csv('label.csv', sep = ',')
# err = rep(NA, length(subFiles))
# for (met in 1:length(subFiles)){
# 
#     prediction = read.csv(subFiles[met], sep = ',')
#     yp = merge(prediction, label, by = 'id', all.y = TRUE)
#     err[met] = with(yp, logLoss(y, prob))
#     
# }
# 
# #########################################################################
# write.table(err, file = 'proj_3.csv', sep = ',', row.names = FALSE,
#             col.names = FALSE)
# write.table(run.time, file = 'proj_3.csv', sep = ',', 
#             row.names = FALSE, col.names = FALSE, append = TRUE)
```

