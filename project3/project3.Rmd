---
title: "CS598 - Project 3"
author: "Xiaoming Ji"
output: pdf_document
---
# Computer System

## Hardware
- Dell Precision Tower 5810
- CPU: Intel Xeon E5-1607 @ 3.10GHz
- Memory: 32GB
- GPU: Nvidia GeForce GTX 1080 (2 cards)

## Software
- OS: Windows 10 Professional 64bit
- R: 3.5.1
- R Packages:
    - catboost_0.11.1
    - xgboost_0.71.2
    - randomForest_4.6-14
    - glmnet_2.0-16
    - kernlab_0.9-26

```{r, message=FALSE, warning=FALSE, include=FALSE}
mypackages = c("knitr", "kableExtra")   # required packages
tmp = setdiff(mypackages, rownames(installed.packages()))  # packages need to be installed
if (length(tmp) > 0) install.packages(tmp)
lapply(mypackages, require, character.only = TRUE)
```


```{r, eval=FALSE, include=FALSE}
data.all = read.csv("loan_stat542.csv")
test.id.all = read.csv("Project3_test_id.csv")

for (i in 1:(dim(test.id.all)[2])){
  
  train.data = data.all[!data.all$id %in% test.id.all[,i],]
  write.csv(train.data, file=paste("train", i, ".csv", sep=""), row.names = FALSE)
  test.data = data.all[data.all$id %in% test.id.all[,i],]
  
  label.data = test.data[,c("id", "loan_status")]
  colnames(label.data)[2] = 'y'
  label.data$y[label.data$y == "Charged Off"] = "Default"
  label.data$y = as.numeric(label.data$y)
  label.data$y[label.data$y == 3] = 0
  label.data$y[label.data$y == 2] = 1
  write.csv(label.data, file=paste("label", i, ".csv", sep=""), row.names = FALSE)
  
  test.data = test.data[, c("loan_status") != names(test.data)]
  write.csv(test.data, file=paste("test", i, ".csv", sep=""), row.names = FALSE)
}
```

## Preprocessing and Feature Engineering

Several approaches are taken to pre-process the data.

- Response label: merge *Charged Off* with *Default* and convert label values to 0 or 1.
- Build new predictors to help training/prediction: 
    - `earliest_cr_line_mon`: derived from *earliest_cr_line* that indicates how many months has elapsed till *2019-1-1* when the borrower's earliest reported credit line was opened.
    - `fico_score`: consolidate *fico_range_high* and *fico_range_low* using formula: *(fico_range_high + fico_range_low) / 2*.
- Level grouping: 
    - `zip_code`: it has more than 900 levels, I group these values to 10 new levels to reduce memory usage and improve performance.
- Remove predictors: remove less useful and redundant predictors.
    - `emp_title` (too many levels), `title` (redundant with *purpose*), `grade` (redundant with *sub_grade*) ,`earliest_cr_line`, `fico_range_high`, `fico_range_low`.

## Models

For testing purpose, I build 7 models,

- Dumb model: just predict 0.2 for every sample.
- Logistic Regression
- Boosting (XGBoost, CatBoost)
- RandomForest
- Lasso
- liner SVM

Suprisingly, Dumb model can achieve `0.504` logloss score. kernlab *ksvm()* fails to build the model (hang forever).  lasso and random forest don't give me significant improvement than logistic regression and they take much longer time to build. Thus, I will pick `dumb`, `Logistic Regression` and `Boosting` as my final models. 

**Note**: My testing shows CatBoost performs at least 10x faster than XGBoost (even without GPU). In case catboost library is not installed, xgboost will be used.

## Evaluation

I tested all 3 test datasets against these models with the parameters,

- Dumb: None
- Logistic Regression: liner with all predictors.
- Boosting: learning_rate = 0.09, iterations = 1200.

The LogLoss scores are,

```{r, echo=FALSE, message=FALSE, warning=FALSE}
load("EVAL.OBJ")

test.results = rbind(eval.obj$loss, Average=colMeans(eval.obj$loss))
kable(test.results) %>%
  kable_styling(latex_options = "striped")
```

```{r eval=FALSE, warning=FALSE, include=FALSE}
#########################################################################
# Test code begins

start.time = Sys.time()

LOGLOSS = matrix(0, 3, 3)
rownames(LOGLOSS) = c("Test1", "Test2", "Test3")
TEST_NUM = 0

for (i in 1:3){
  TEST_NUM = i
  TRAIN_FILE_NAME = paste("train",i, ".csv", sep = "")
  TEST_FILE_NAME = paste("test",i, ".csv", sep = "")
  LABEL_FILE_NAME = paste("label",i, ".csv", sep = "")
  source('mymain.R')
}
end.time = Sys.time()
run.time = as.numeric(difftime(end.time, start.time, units = 'secs'))

print(LOGLOSS)
cat("\nComputation time:", ceiling(run.time), "Seconds")

eval.obj = list(loss=LOGLOSS, compute.time=run.time)
save(eval.obj, file="EVAL.OBJ")
```

Computation time: `r ceiling(eval.obj$compute.time)` seconds